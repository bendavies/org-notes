* TODO 1 Recurrent Problems [3/3]
** DONE 1.1 The Tower of Hanoi
   CLOSED: [2015-01-21 Wed 00:22]

   Given disks 0..n-1, we have three poles s, e, a. Let H(n, s, e, a)
   be the problem of moving 0..n-1 disks from pole s to pole e.

   We know that at some point in time we need to move disk n-1 from s to e.
   We know that to move disk k from s to e, disk k has to be the top
   disk on pole s, and disks on pole e must be greater than disk k.

   We know that we have to move the n-1th disk from s to e at some point.

   We know that the fastest way to move the n-1th disk from s to e is to
   move the n-1th disk from s to e (as opposed to, say, moving it from
   s to a and then from a to e).

   We have to do:
   $H(n, s, e, a)$ is

   1. H(n-1, s, a, e)
   2. Move disk n-1 from s to e
   3. H(n-1, a, e, s)

   And have this recurrence for the number of moves:

   - *T_0 = 0, T_n = 2 * T_{n-1} + 1* for n > 0

   Consider
   - T_0 + 1 = 1
   - T_n + 1 = 2(T_{n-1} + 1)

   Let $U_n = T_n + 1$. Then

   - U_0 = 1
   - U_n = 2(U_{n-1}).

   Hence we get U_n = 2^n and T_n = 2 2^n - 1

** DONE 1.2 Lines in the plane
   CLOSED: [2015-01-21 Wed 07:53]

   - L_0 = 1
   - L_1 = 2

   Given n lines, there can at least 0 lines that are parallel to
   each other. If there are no lines that are parallel to each other,
   then every line intersects with each other.

   Let us consider n-1 nonparallel lines that split L_{n-1} regions. When we add
   the nth line, it may or may not be parallel to an existing line. If it is
   parallel, then the nth line intersects with n-2 lines creating n-1 new
   regions. But the nth line can always be constructed such that it is not
   parallel with any existing line and any existing intersection. Then it
   intersects with n-1 lines creating n new regions. In addition, we know
   that to create n new regions a line needs to split n old regions; in
   order for a line to split n old regions the line needs to pass through
   them; in order for a new line to pass through n old regions there needs
   to be n-1 lines that split the new line into n sections. Hence given
   n-1 lines, at most n new regions can be created.

   Hence we have

   - *L_0 = 1;*
   - *L_n = L_{n-1} + n* for n > 0.

   We note that

   L_n = 1 + S_n, where S_n = 1 + 2 + ... + n

   S_n are the triangular numbers. We know that S_n has the closed
   form

   - *S_n = n(n+1) / 2*, for n >= 0.

   Given L_n = n(n+1)/2 + 1, we can prove the closed form from the recurrence

   L_n = L_{n-1} + n = ((n-1)n)/2 + 1 + n = (n^2 - n + 2n)/2 + 1 =
   (n^2 + n)/2 + 1 = n(n+1)/2 + 1

   - *L_n = n(n+1)/2 + 1*

   A more detailed proof will explicate the definition of intersections,
   consider the cases where a new line intersects with an existing intersection,
   and create a relationship between the number of regions and the number
   of intersections.

   Consider Z_n, where Z_n is the max number of regions that n lines with
   a 'zig' in them can divide.

   Z_0 = 1; Z_1 = 2; Z_2 = 7

   Let us consider the nth zigged line. It consists of a left part and a
   right part. Beyond the first line that intersects with the zig, each
   part intersects with n-1 regions forming in total 2(n-1) new regions.
   The zig intersects with first line forming 1 new region. We have
   the recurrence

   - *Z_0 = 1*
   - *Z_n = 2*Z_{n-1} + 1*

   Alternatively, we can also reduce Z to the case of L. Each new zigged
   line can be considered as 2 lines, but forming 2 fewer regions than L
   lines. Hence we can have

   Z_n = L_{2n} - 2n = 2n(2n+1)/2 + 1 - 2n = 2n^2 - n + 1 for n >= 0
   - *Z_n = 2n^2 - n + 1*

** DONE 1.3 The Josephus Problem
   CLOSED: [2015-01-21 Wed 08:59]

   Consider J(2n). After 1 round, all even numbers are eliminated,
   and n people are left, such that the ith person used to be the
   (2i-1)th person. Let J(n) = k. Then J(2n) = 2k-1 = 2J(n) - 1

   Consider J(2n+1). After 1 round, all even numbers are eliminated
   as well as 1, and n people are left, such that the ith person used
   to be the (2i+1)th person. Let J(n) = k. Then J(2n+1) = 2J(j) + 1.

   Hence we have

   - *J(1) = 1*
   - *J(2n) = 2J(n) - 1*
   - *J(2n+1) = 2J(n) + 1*, n >= 1

   Consider 2^n. We have J(2^(n)) = 2J(2^(n-1)) - 1 = 2 * 1 - 1 = 1.

   Consider J(2^n + l), such that 0 <= l < 2^n. This means that after
   l executions of every other index, the survivor is 1.

   Hence we have

   - *J(2^n + l) = 2l + J(2^n) = 2l + 1*

   - We can also model the problem as binary digit transformations.

     Let n = (b[m] b[m-1] ... b[0])_2. Then b[m] = 1. Then
     l = (b_{m-1} ... b_0)_2 and J(n) = 2l+1 = (b_{m-1} ... b_0 1)_2 =
     (b_{m-1} ... b_0 b_m), which looks like a cyclic shift, but where
     initial 0s are eliminated.

   Repeated applications of J reaches a fixed point where J(n) = n; this
   happens when n satisfies n = 2^m - 1. Let \nu(n) be the number of 1
   bits in n. Then for all positive J(n), J^{\infty}(n) = 2^{\nu(n)} - 1

   Consider J(n) = n / 2. Then n = 2^m + l and l = (2^m - 2) / 3

   N has the bit pattern (1 0 ... 1 0)_2. These are the numbers such that
   a cyclic shift left is the same as a cyclic shift right (halving).

   - Generalizing J, we have

     - *f(1) = \alpha;*
     - *f(2n) = 2f(n) + \beta* for n >= 1;
     - *f(2n+1) = 2f(n) + \gamma* for n >= 1.

     Then, f(n) can be modeled as a linear combination of multiples of
     \alpha, \beta and \gamma. We can write

     - f(n) = A(n)\alpha + B(n)\beta + C(n)\gamma.

     Let \alpha = 1, \beta = \gamma = 0. We have

     f(1) = 1
     f(n) = A(n)
     f(2n) = A(2n) = 2f(n) = 2A(n) implies A(2n) = 2A(n)
     f(2n+1) = A(2n+1) = 2f(n) = 2A(n) implies A(2n+1) = 2A(n)

     - We can easily hypothesize and prove that A(2^m+l) = 2^m

     - We try f(n) = 1. Then from f(1) = \alpha we get *\alpha = 1*.
     - From f(2n) = 2f(n) + \beta we get 1 = 2 + \beta. Then *\beta = -1*.
     - From f(2n+1) we get *\gamma = -1*.

     We have f(n) = 1 = A(n) - B(n) - C(n)

   We try f(n) = n. We get \alpha = 1, \beta = 0, \gamma = 1.

   Note that we try values of f(n) such that the recurrences are easily
   computed.

   We have f(n) = n = A(n) + C(n)

   Hence we have C(n) = n - 2^m = l

   Hence we have B(n) = 2^m - l - 1

   - Considering binary representations,

   f(1) = \alpha, f(2n + j) = 2f(n) + \beta[j], for j = 0, 1

   (\beta[0] = \beta, \beta[1] = \gamma)

   We have

   f((b[m] b[m-1] ... b[0])[2]) = 2f((b[m] b[m-1] ... b[1])[2]) + \beta[b[0]]
   = 2^m \alpna + 2^[m-1] \beta[b[m-1]] + ... + \beta[b[0]]

   We can write

   f((b[m] b[m-1] ... b[1] b[0])[2]) = (\alpha \beta[b[m-1]] ... \beta[b[0]])[2]

   - Consider the even more general recurrence

   f(j) = \alpha[j], for 1 <= j < d
   f(dn+j) = cf(n) + \beta[j], for 0 <= j < d and n >= 1

   We have

   f((b[m] ... b[0])[d]) = (\alpha[b[m]] \beta[b[m-1]] ... \beta[b[0]])[c]

* TODO 2 Sums [1/2]
** DONE 2.1 Notation
   CLOSED: [2015-01-20 Tue 23:56]
   - a_1 + a_2 + ... + a_n
   - \sum_{k=1..n}(a_k)
   - \sum_{1<=k<=n}(a_k)
   - \sum_{p<=N; p prime}(1/p)
   - General notation: P(k) is a constraint
     on integers (maps to boolean): \sum_{P(k)}(a_k)
   - Iversonian notation: [p prime]
   - Rewriting constraints on integers summed
     into using Iversonian: \sum_{k}(a_k [P(k)])

** TODO 2.2 Sums and Recurrences

   - Expressing sums as recurrences
     S_n = \sum_{k=0..n}(a_k) is equivalent to

     S_0 = a_0; S_n = S_(n-1) + a_n for n > 0

   - *Consider the recurrence where a[n] = \beta + \gamma n.* We then have the general
     form

     R_0 = \alpha ; R_n = R_{n-1} + \beta + \gamma n for n > 0

     We can infer that R[n] is a linear combination of \alpha, \beta, and
     \gamma. Hence we know that there exist functions A, B and C such that

     R_n = A(n)\alpha + B(n)\beta + C(n)\gamma.

     -
       *We can employ the repertoire method* and plug in values for \alpha,
       \beta and \gamma to find A, B and C.

       Consider R_n = 1; we have \alpha = 1, \beta = \gamma = 0, therefore A(n) = 1

       Consider R_n = n; we have \alpha = \gamma = 0, \beta = 1, therefore B(n) = n

       we know that

       - R_n = \alpha + n\beta + C(n)\gamma
       = \alpha + (n-1)\beta + C(n-1)\gamma + \beta + \gamma n

       We know that C(0) = 0.

       hence C(n)\gamma = C(n-1)\gamma + \gamma n and C(n) = C(n-1) + n for n > 0,
       hence C(n) = n(n+1) / 2

       Hence Sum_{k=0..n}(a+bk) = R_n where \alpha = \beta = a and \gamma = b;
       hence Sum_{k=0..n}(a+bk) = a + an + bn(n+1)/2 = a(n+1) + b(n+1)n/2

   - Now consider the Tower of Hanoi problem. We can reduce it to this case.

     Consider

     - $T_0/2^0 = 0$
     - $T_n/2^n = T_{n-1}/2^{n-1} + 1/2^n, for n > 0$

     Set S_n = T[n]/2^[n]. Then S[0] = 0; S[n] = S[n-1] + 2^[n-1], for n > 0

     S_n = Sum[k=1..n](2^[-k]). This is a geometric series. We know that

     S_n = 1 - 2^{-n}. Hence T_{n} = 2^n S_n = 2^n - 1

   - We consider a general recurrence T[0] = c[0];
     a[n]T[n] = a[n]T[n-1] + c[n], n > 0.

     Consider s[n] such that s[0]a[0] = 1 and s[n-1]a[n-1] = s[n]b[n] for n > 0

     Consider

     s[n]a[n]T[n] = s[n]b[n]T[n-1] + s[n]c[n]

     We can write, letting S[n] = s[n]a[n]T[n]

     S[n] = S[n-1] + s[n]c[n]

     S[n] = s[0]a[0]T[0] + Sum[k=1..n](s[k]c[k])
     = s[1]b[1]T[0] + Sum[k=1..n](s[k]c[k])

     T[n] = (a[1]b[1]T[0] + Sum[k=1..n](s[k]c[k]) / (s[n] a[n])

     Through the recurrence of s[n], we have

     s[n] = Prod[k=1..n-1](a[k] / b[k+1]).

     This method only works for a, b > 0, and when
     Sum[k=1..n](s[k]c[k]) has a closed form.

   - The average number of comparison steps made by quicksort on n items
     in random order satisfies the recurrence

     - *C_0 = 0;*
     - *C_n = n + 1 + 2/n \sum_{k=0..n-1} C_k* for n > 0.

     Perbutating the recurrence,

     nC_n = n^2 + n + 2 \sum_{k=0..n-1} C_k for n > 0

     (n-1)C_{n-1} = (n-1)^2 + (n-1) + 2 \sum_{k=0..n-2} C_{k} for n-1 > 0

     nC_n - (n-1)C_{n-1} = 2n + 2C_{n-1}

     nC_n = (n+1)C_{n-1} + 2n for n > 0

     Let a_n = n, b_n = n+1 and c_n = 2n. We have
     s_n = \prod_{k=1..n-1}(n/(n+2)) = 2 / n(n+1)

     Finally, we have the closed form
     - *C_n = 2(n+1) \sum_{k=1..n} 1/(k+1)*

   - Consider the harmonic series *H_n = Sum_{k=1..n}(1/k)*.

     - *C[n] = 2(n+1) ( H[n] - 1 + 1/(n+1) ) = 2(n+1)H[n] - 2n*

** DONE 2.3 Manipulation of sums

   - distributive: *\sum[k](ca[k]) = c \sum[k](a[k])*
   - associative: *\sum[k](a[k] + b[k]) = \sum[k](a[k]) + \sum[k](b[k])*
   - commutative: *\sum[k](a[k]) = \sum[p(k)](a[p(k)]) where p is bijective in Z

   Arithmetic progression:

   - *Sum_{k=0..n}(a+bk) = (a+bn/2)(n+1)*

   Sum of k in sets:
   - [k \in K] + [k \in K'] = [k \in K \cap K'] + [k \in K \cup K']

   Pertubation method: the general idea is to find a substructure
   in the sum that occurs f(N) times, where f is a multiple/base
   of N.

   - Consider a geometric progression
     - *S_n = \sum_{0<=k<=n}(ax^k)*

     Then, S[n] + ax^[n+1] = ax[0] + Sum[0<=k<=n] ax^[k+1] = ax + xS[n].
     Hence (1-x)S[n] = a - ax^[n+1] and

     - *S_n = (a - ax^[n+1]) / (1-x)*

   - Consider
     - *S_n = Sum{0<=k<=n}(k2^k)*

     Consider S[n+1] = S[n] + (n+1)2^[n+1] = (-0) + Sum[0<=k<=n]((k+1)2^[k+1])

     Sum[0<=k<=n]((k+1)2^[k+1]) = Sum[k=0..n](k2^k) + Sum[k=0..n](2^[k+1])
     = 2S[n] + 2^[n+2] - 2

     Therefore S_n + (n+1)2^{n+1} = 2S_n + 2^{n+2}-2, hence

   - S_n = (n-1)2^{n+1}+2

   Generalizing 2 to x, we can, through a similar derivation, have

   - *\sum_{k=0..n}(kx^k) = (x-(n+1)x^{n+1}+nx^{n+2}) / (1-x)^2*

   - We could also have obtained this result by taking the derivative of

     - *\sum_{k=0..n}(x^k) = (1-x^{n+1})/(1-x)*

** DONE 2.4 Multiple sums

    - Summing over all pairs of j and k:
      - \sum_{P(j,k)} a_{j,k}
      - \sum_{j,k} a_{j,k}[P(j,k)]

    - we can interchange the order of summation

    - Consider
      - *\sum_{1<=j,k<=3} a_jb_k*

      equal to
      - (\sum_{j=1..3}a_j)(\sum_{k=1..3}b_k)
    - In general,
      - *\sum_{j \in J; k \in K} a_jb_k = (\sum_{j \in J} a_j )( \sum_{k \in K} b_k )*

    - vanilla interchange of summation order
      - *\sum_{j\in J}\sum_{k\in K} a_{j,k} = \sum_{k\in K}\sum_{j \in J} a_{j,k}*

    - rocky-road
      - \sum_{j\in J}\sum_{k\in K(j)} a_{j,k} = \sum_{k\in K'}\sum_{j\in J'(k)} a_{j',k'}
      - where [j \in J][k \in K(j)] = [k' \in K'][j' \in J'(k')]
      - While we can let J = K' and K(j) = J'(k'), there are special cases such as
        - [1 <= j <= n][j <= k <= n] = [1 <= k <= n][1 <= j <= n]

    - consider matrix
      - (a_{i,j})_{p \times q}

      Consider the sum of the upper triangular of the matrix
      - *S = \sum_{1<=j<=k<=n}a_ja_k*

      then

      2S = \sum_{1<=j,k<=n} a_j a_k + \sum_{1<=j=k<=n} a_j a_k

      hence

      - *S = (1/2)((\sum_{k=1..n} a_k)^2 + \sum_{k=1..n} a_k^2)*

    - Consider
      - *S = \sum_{a<=k<j<=n} (a_k-a_j)(b_k-b_j)*

      we can interchange the j and k indices (and add the sums together)

      2S = \sum_{1<=j,k<=n} (a_j-a_k)(b_j-b_k) - \sum_{1<=j=k<=n} (a_j-a_k)(b_j-b_k)

      second sum is zero; we expand the term in the first sum to get

      - *S = n \sum{k=1..n} a_k b_k - (\sum{k=1..n} a_x)(\sum_{k=1..n} b_k)*

      which are Chebyshev's monotonic inequalities

    - \sum_{j\in J} a_{f(j)} = \sum_{k\in K} a_k * #f^{-}(k)
      - where f^{-} maps the reverse of f.

    - Consider
      - S_n = \sum_{1<=j<k<=n} 1/(k-j)

      - *S_n = \sum_{0<=k<n} H_k*

      Next, we try summing by diagonals

      S_n = \sum{1<=j<k+j<=n} 1/k

      - *S_n = nH_n - n*

      - \sum_{0<=k<n} H_k = nH_n - n

** TODO 2.5 General methods

   General methods for solving a sum

   Consider finding a sum for S_n = \sum_{0<=k<=n} k^2 = n(n+1)(2n_1)/6

   1. Look it up
      - Try the CRC Standard Mathematical Tables
      - Handbook of Mathematical Functions (Abramowitz and Stegun)
      - Handbook of Integer Sequences (Sloane)
      - Use a symbolic manipulation program
   2. Guess the answer, prove it by induction
   3. Perturb the sum
   4. Build a repertoire
   5. Replace sums by integrals
      Obtain the integral solution, then use a recurrence to determine
      the error between the integral solution and the discrete solution
   6. Expand and contract
      Replace the original sum with a double sum that can be simplified if
      massaged properly (if we have solutions for part of the double sum)
   7. Use finite calculus
   8. Use generating functions

** TODO Finite and infinite calculus

   - \Delta(f(x)) = f(x+1) - f(x)

   \Delta is an operator: operators operate on functions to give new functions.

   - x^{u(m)} = \prod_{i=0..m-1}(x-i), integer m >= 0

   - x^{o(m}) = \prod_{i=0..m-1}(x+i), integer m >= 0

   x^{o(m)} = (x+m-1)^{u(m)}

   We call these falling factorial powers and rising factorial powers
   respectively.

   We have

   - *\Delta(x^ubar[m]) = mx^ubar[m-1]*

   - *g(x) = Delta(f(x)) iff Sum(g(x)\delta(x) = f(x) + C*

   C need not be constant, but has to beling to the class of 1-periodic functions,
   i.e. C(x) = C(x+1)

   - *\sum_a^b g(x) \delta x = f(x) |_a^b = f(b) - f(a) = \sum_{a<=k<b} g(k)

   - sums of falling powers
     - *\sum_{0<=k<n} k^{u(m)} = n^{u(m+1)}/(m+1)

   - ordinary powers, factorial powers, and Stirling numbers

     - k = k^{u(1)}
     - k^2 = k^{u(2)} + k^{u(1)}
     - k^3 =k^{u(3)} + 3k^{u(2)} + k^{u(1)}

   - generalization to negative falling powers
     - *x^{u(-m)} = \prod_{i=1..m} 1/(x+i)*
   - falling-power law of exponents
     - x^{u(m+n)} = x^{u(m)}(x-m)^{u(n)}

   TODO

** TODO 2.7 Infinite sums

   - bounding constant

* TODO Integer Functions [1/1]

** DONE Floors and Ceilings
   CLOSED: [2015-01-21 Wed 13:25]

  - fl(x), ce(x)
  - *fl(x) = x iff ce(x) = x iff x in Z*

  \|ce(x) - fl(x)| = [x not in Z]


  - *floor(x) = -ceil(-x)* and ceil(x) = -floor(-x)

  - *floor(x) <= x < floor(x) + 1*
  - x - 1 < fl(x) <= x
  - ce(x) - 1 < x <= ce(x)
  - x <= ce(x) < x + 1
  - fl(x + n) = fl(x) + n for n \in Z
  - *n1 <= floor(x) < n2 iff n1 <= x < n2*
  - n1 < ceil(x) <= n2 iff n1 < x <= n2
  - \{x\} = x - floor(x)

** TODO 3.2 Floor/Ceiling Applications

   To write a number n we need

   - *m = fl(lg n) + 1* bits for n > 0
   also, m = ce(lg (n+1))

   - Consider
     - *m = floor(sqrt(fl(x)))*.

     Then m <= sqrt(fl(x)) < m+1.
     Then m^2 <= fl(x) < m^2 + 2m + 1. Then m^2 <= x < m^2 + 2m + 1
     Then m <= sqrt(x) < m + 1. Then m <= fl(sqrt(x)) < m+1.
     - Then *m = fl(sqrt(x))*

   We can generalize beyond square roots. For continuous, monotonic increasing
   f, we have
   - *fl(f(x)) = fl(f(fl(x)))*
   - *ce(f(x)) = ce(f(ce(x)))*

   An important special case is:

   - *f_{m,n}(x) = (x + m) / n* for integers m >= 0 and n > 0

   - Levels of problems in mathematics
     - Level 1:
     - Level 2:
     - Level 3:
     - Level 4:
     - Level 5:

   For integral n,

   - *\alpha a <= n < \beta iff ceil(\alpha) <= n < ceil(\beta)*
   - \alpha < n <= \beta iff floor(\alpha) < n <= floor(\beta)

   - [\alpha .. \beta] contains fl(\beta) - ce(\alpha) + 1
   - [\alpha .. \beta) contains ce(\beta) - ce(\alpha)
   - (\alpha .. \beta] = fl(\beta) - fl(\alpha)
   - (\alpha .. \beta) = ce(\beta) - fl(\alpha) - 1

   Among 1 < n <= 1000, how many n satisfy floor(root3(n)) \ n ?

   Let that number be W. Then

   - W = \sum_{n=1..1000}([n is a winner])
   - = \sum_{1<=n<=1000} [floor(root3(n)) \ n]
   - = \sum_{k, m, n} [1 <= n <= 1000] [n = km] [k = floor(root3(n))]
     - we know that there is only at most one combination of k, m, n that
       satisfies the latter 2 Iversonians
   - = \sum_{k, m, n} [k^3 <= n < (k+1)^3] [n = km] [1 <= n <= 1000]

   We know that k >= 1. Let k = 10. Then the Iversonians evaluate to 1 when
   m = 1 and n = 1000. We consider k >= 10 separately. Then [1 <= n <= 1000]
   becomes redundant.

   W = 1 + \sum_{k, m} [1 <= k < 10] [k^3 <= km < (k+1)^3]
   = 1 + \sum_{k, m} [1 <= k < 10] [m \in [k^2 .. (k+1)^3/k)]
   = 1 + \sum_{k} [1 <= k < 10] ( ceil((k+1)^3/k) - ceil(k^2) )
   = 1 + \sum_{k} [1 <= k < 10] (3k + 4) = 1 + (7 + 31) * 9 / 2 = 172


   Consider a general N, where a winning number K satisfies K = floor(root3(N)).

   Then W = TODO

   - Spec(\alpha) = { floor(k \alpha) | k in Z+ }
     - proof sketch that \alpha /= \beta implies Spec(\alpha) /= Spec(\beta)
       - find the difference between \alpha and \beta
       - find the mth element that the difference is greater than an integer
       - then the subset of the multiset considering elements smaller than or
         equal to Spec(\alpha)'s mth element will be different.

   - We consider Spec(sqrt(2)) and Spec(2+sqrt(2))

   TODO

** TODO 3.3 Floor/ceiling recurrences
** TODO 3.4 'mod': the binary operation

   n = m*fl(n/m) + n mod m
   - *x mod y = x - y*fl(x/y)*
   - x mod 0 = x
   - *c(x mod y) = (cx) mod (cy)*

** TODO 3.5 Floor/ceiling sums

   Consider
   - \sum_{0<=k<n} fl(sqrt(k))
     - we introduce m = fl(sqrt(k))
   - = \sum_{k, m >= 0} m[k<n][m=fl(sqrt(k))]
   - = \sum_{k, m >= 0} m([m^2 <= k < (m+1)^2 <= n] + [m^2 <= k < n < (m+1)^2])

* TODO Number Theory [0/10]
** TODO Divisability
   - *m \ n* iff m > 0 and n = mk for some integer k
   - *gcd(m, n) = max{k | k\m and k\n }* for m + n > 0
   - *lcm(m, n) = min{k | k > 0, m\k and n\k}* for m, n > 0

   If k\m and k\n, there exists a, b such that ka = m and kb = n.
   Let r = n mod m. Then exists integer q such that n = r + qm. Then
   kb = r + qm => kb = r + q(ka). There exists k(b-qa) = r, where
   b-qa is integral. Hence k\r.

   Hence we know that gcd(m, n) <= gcd(n mod m, m)

   Let r = n mod m. Let k\r and k\m. We have n = r + qm. So we have k\n as
   well.

   So gcd(n mod m, m) <= gcd(m, n). Therefore gcd(m, n) = gcd(n mod m, m).

   - Euclid's algorithm
     We know that
     - *gcd(0, n) = n* and
     - *gcd(m, n) = gcd(n mod m, m)*, for m > 0

   Let d be any common divisor of m and n. Then d\(m'm+n'n). Then there exists
   a, d=(1/a)(m'm+n'n).

   We can also compute integers m', n' such that
   m'm + n'n = gcd(m,n).

   We know that m'=0 and n'=1 exists for gcd(0, n). Given r'r+m'm = gcd(r, m)
   we have r'r+m'm = r'(n-m*floor(n/m))+m'm = r'n + (m'-m*floor(n/m))m =
   gcd(n, m)

   Let Sum(m\n) a[m] denote the sum of terms a indexed by divisors of n.
   We also have Sum[m\n] a[m] = Sum[m\n] a[n/m].

   From the definition of divisibility, we also have
   Sum[m\n] a[m] = Sum[k]Sum[m>0] a[m] [n=mk]

   We also have
   Sum[m\n] Sum[k\m] a[k,m] = Sum[k\n] Sum[l\(n/k)] a[k,kl]

   (We can prove this by considering the Iversonian form of the sums)

   - [ ] Proof using Iversonian

** TODO Primes
** TODO Prime Examples
** TODO Factorial Factors
** TODO Relative Primality
** TODO 'mod': The Congruence Relation
** TODO Independent Residues
** TODO Additional Applications
** TODO Phi and Mu
** TODO Exercises
* TODO Binomial Coefficients [0/0]
* TODO Special Numbers [0/0]
* TODO Generating Functions [0/0]
* TODO Discrete Probability [0/0]
* TODO Asymptotocs [0/0]
