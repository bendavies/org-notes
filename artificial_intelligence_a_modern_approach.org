* TODO I Artificial intelligence
** TODO 1 Introduction
*** TODO 1.1 What is AI?
**** TODO Acting humanly: the Turing test approach
     Present CS mostly interested in this when required that AI behave like
     normal human.
**** TODO Thinking humanly: the cognitive modelling approach
     c.f. cognitive science
**** TODO Thinking rationally: the laws of thought approach
     c.f. logic, which is limited in expression. The logicist tradition.
**** TODO Acting rationally: the rational agent approach
*** TODO 1.2 The foundations of artificial intelligence
**** TODO Philosophy (428 B.C. - present)
**** TODO Mathematics (c. 800 - present)
**** TODO Psychology (1879 - present)
**** TODO Computer engineering (1940 - present)
**** TODO Linguistics (1957 - present)
*** TODO 1.3 The history of artificial intelligence
**** TODO The gestation of artificial intelligence (1943 - 1956)
**** TODO Early enthusiasm, great expectations (1952 - 1969)
**** TODO A dose of reality (1966 - 1974)
**** TODO Knowledge-based systems: the key to power? (1969 - 1979)
**** TODO AI becomes an industry (1980 - 1988)
**** TODO The return of neural networks (1986 - present)
**** TODO Recent events (1987 - present)
*** TODO 1.4 The state of the art
*** TODO Bibliographical and historical notes
*** TODO Exercises
*** TODO 1.5 Summary
** TODO 2 Intelligent agents
*** TODO 2.1 Introduction
    agent: perceiving through sensors and acting through effectors
    - agent
    - perceiving
    - sensors
    - acting
    - effectors
*** TODO 2.2 How agents should act
    rational agent: performance measure

    What is rational depends on
    - The performance measure
    - The percept sequence :: everything the agent has perceived so far
    - What the agent knows about the environment (possibly the percept space?)
    - The actions the agent can

    Ideal rational agent: for each possible percept sequence, an ideal
    rational agent should do whatever action is expected to maximize its
    performance measure, on the basis of the evidence provided by the
    percept sequence and whatever built-in knowledge the agent has.

    - rational agent
    - performance measure
    - percept sequence
    - ideal rational agent
**** TODO The ideal mapping from percept sequences to actions

    As a corollary, ideal mappings from percept sequence to actions
    describe an ideal agent. We do not need to create large tables mapping
    percepts to actions if there are regularities in the mapping (e.g. sqrt
    function)

**** TODO Autonomy

    - autonomy :: the ability of an agent to take a different action based
                  on a different percept sequence. (An agent without autonomy
                  acts completely on built-in knowledge.

*** TODO 2.3 Structure of intelligent agents
    - agent program: a function that implements the agent mapping from
      percepts to actions
    - architecture
    - software agents
    - PAGE: (Percepts, Actions, Goals (performance measure), Environment)
**** TODO Agent programs
     AI designs the agent program that runs on a computing device, called the
     architecture.
     The agent program receives only a single percept at a time as input.

     #+BEGIN_SRC
     function Skeleton-Agent(percept) returns action
     #+END_SRC

**** TODO Why not just look up the answers?
     The problem space may be stupid large. Also, no learning mechanism
     for novel situations.

     #+BEGIN_SRC
     function Table-Driven-Agent(percept) returns action
     #+END_SRC

**** TODO An example
     - percepts provided by sensors
     - actions available to a taxi driver
     - performance measure
     - environment

**** TODO Simple reflex agents

     - condition-action rule

     if car-in-front-is-braking then initiate-braking

     #+BEGIN_SRC
     function Simple-Reflex-Agent(percept) returns action
     #+END_SRC

**** TODO Agents that keep track of the world

     - internal state: e.g. the previous frame from the camera, to determine
       through the presence of red lights simultaneously turning on whether
       there exists a braking car in front.

     This is required because sensors do not provide access to the complete
     state of the world.

     #+BEGIN_SRC
     function Reflex-Agent-With-State(percept) returns action
     #+END_SRC

**** TODO Goal-based agents
     Chooses actions that achieve the goal. (assumption that all actions
     that can lead to the goal will lead to the goal in finite time.)
     The goal based agent can work with novel conditions that may not be
     programmed in a simple reflex agent.

     - goal information
     - search
     - planning

**** TODO Utility-based agents

     Also cares that the action leads to the goal as quick as possible.

     - utility
     - explicit utility function

*** TODO 2.4 Environments
**** TODO Properties of environments
     - accessible vs inaccessible
       - accessible: sensors give complete stte of environment
     - deterministic vs nondeterministic
       - deterministic: next state of environment completely determined by
         current state
     - episodic vs nonepisodic
       - episodic: environment divided into turns; future turns do not depend
         on previous turns (e.g. multiple chess games); analysing multiple
         images
     - static vs dynamic
       - dynamic: environment can change while agent is deliberating
       - semidynamic :: environment doesn't change bit agent's performance score
                       does
     - discrete vs continuous
       - discrete: limiited number of distinct precepts
**** TODO Environment programs
     Programs that create an environment for agents. (c.f.
     frameworks in software)

     - environment class :: rather than against a particular chess player (one
         player), we design a chess AI to play against a class of players

     #+BEGIN_SRC
     procedure Run-Environment(state, UPDATE-FN, agents, termination)
     #+END_SRC

     #+BEGIN_SRC
     Run-Eval-Environment(state, UPDATE-FN, agents, termination, PERFORMANCE-FN) returns scores
     #+END_SRC

*** TODO 2.5 Summary
    - agent
    - ideal agent
    - autonomous
    - agent program
    - basic agent program designs
    - reflex agents
    - goal-based agents
    - utility-based agents
    - making decisions by reasoning with knowledge
    - properties of environments
*** TODO Bibliographical and historical notes
*** TODO Exercises
* TODO II Problem-solving
** TODO 3 Solving problems by searching
   - problem-solving agent
*** TODO 3.1 Problem-solving agents
    - goal formulation :: what state/function to achieve based on the current
                          situation
    - problem formulation :: process of deciding what actions and states to
                             consider.
    - search :: function that takes a problem as input and returns a solution
                in the form of an action sequence

    - exploration problem :: where an agent has no information about the
         effects of its actions. (it must learn a map of its environment)


    - goal formulation
    - problem formulation
    - search
    - solution
    - execution

    #+BEGIN_SRC
    function Simple-Problem-Solving-Agent(p) returns an action
    #+END_SRC

*** TODO 3.2 Formulating problems
**** TODO Knowledge and problem types
     - single-state problem :: a problem where an agent always knows what
          state it is in
     - multiple-state problem :: a problem where an agent knows that it is
          in one on multiple states, but doesn't know which

     - contingency problem :: each branch of the state tree deals with a
         possible contingency that might arise. They require an agent
         design in which the agent can act before it has found a guaranteed
         plan. (It might be able to start executing and seeing what
         contingencies might arise)

     Where an agent can expect to encounter contingency problems, it can
     interleave search and execution.

     - single-state problem
     - multiple-state problem
     - contingency problem
     - interleaving of search and execution
     - exploration problem

**** TODO Well-defined problems and solutions
     To define a problem we need

     - The initial state of an agent
     - The set of possible actions and the states they result in

     This defines the state space of the problem: the set of all states
     reachable from the initial state

     - The goal test; a function that checks if a state is a goal

     - A path cost

     We can define a problem as consisting of initial-state, operators,
     goal-test, and path-cost-function

     The state space is replaced by the state set space.

     - probem: collection of information for agent to decide what to do
     - initial state
     - operator: maps states to states after action carried out
     - successor function: successor of a state
     - state space: set of all states reachable from initial
     - path: sequence of actions
     - goal test
     - path cost

     #+BEGIN_SRC
     datatype PROBLEM
       components: INITIAL-STATE, OPERATORS, GOAL-TEST, PATH-COST-FUNCTION
     #+END_SRC

**** TODO Measuring problem-solving performance

     - search cost :: cost associated with time and memory required to find a
                      solution.
     - total cost :: sum of the path cost and the search cost. (path cost is
                     the cost of taking the path)

**** TODO Choosing states and actions
     Abstract out all the irrelevant details into a weighted graph.
     - abstraction
*** TODO 3.3 Example problems
    - toy problems
    - real-world problems
**** TODO Toy problems
     - The 8-puzzle
       - states: TODO
       - operators: TODO
       - goal test: TODO
       - path cost: TODO
       - sliding-block puzzles: NP-complete
     - The 8-queens problem
       - goal: TODO
       - path cost: zero
       - states: TODO
       - operators: TODO

       - incremental formulation
       - complete-state formulation
     - Cryptarithmetic
       - TODO
     - The vacuum world
     - Missionaries and cannibals
**** TODO Real-world problems
     - Route finding
     - Touring and travelling salesperson problems
     - VLSI layout
     - Robot navigation
     - Assembly sequencing
*** TODO 3.4 Searching for solutions
**** TODO Generating action sequences
     Expanding the state: applying operators to the current state (e.g. actions)
     Which state to expand first is called the search strategy.

     - generating a new set of states
     - expanding the states
     - search strategy
     - search tree
     - search node

     #+BEGIN_SRC
     General-Search(problem, strategy) returns a solution, or failure
     #+END_SRC

**** TODO Data structures for search trees
     c.f. building up a search tree superimposed over the state space.

     - A node has
       - the state in the state space to which the node corresponds
       - the parent node
       - the operator that was applied to generate the node
       - the depth
       - the path cost of the path from init to the node

     #+BEGIN_SRC
     datatype node
       components: STATE, PARENT-NODE, OPERATOR, DEPTH, PATH-COSE
     #+END_SRC

     - fringe/frontier
     - queue
       - Make-Queue(Elements)
       - Empty?(Queue)
       - Remove-Front(Queue)
       - Queueing-Fn(Elements, Queue)


     #+BEGIN_SRC
     General-Search(problem, Queuing-Fn) returns a solution, or failure
     #+END_SRC

*** TODO 3.5 Search strategies

    4 criteria

    - completeness
      - If exists solution, will algo find it?
    - time complexity
    - space complexity
    - optimality
      - Does algo find best solution?

    - uninformed search
    - blind search
    - informed search
    - heuristic search

    - search strategies differ by order in which they expand nodes

**** TODO Breadth-first search

     - breadth-first-search

     #+BEGIN_SRC
     function Breadth-First-Search(problem) returns a solution or a failure
       return General-Search(problem, Enqueue-At-End)
     #+END_SRC

     - branching factor
     - memory requirements are a big problem

**** TODO Uniform cost search

     - uniform cost search
       - expands lowest-cost node (generalization of BFS)
**** TODO Depth-first search

     - depth-first search

     #+BEGIN_SRC
     function Depth-First-Search(problem) returns a solution, or failure
       General-Search(problem, Enqueue-At-Front)
     #+END_SRC

**** Depth-limited search
     - cutoff at a certain depth

**** TODO Iterative deepening search
     - diameter: same meaning as graph diameter

     - iterative deepening search

     #+BEGIN_SRC
     function Iterative-Deepening-Search(problem) returns a solution sequence
     #+END_SRC

     In general, IDS is the preferred search method when there is a large search
     space and the depth of the solution is not known


**** TODO Bidirectional search
     Finds an O(b^d) in O(b^(d/2)) time in general.

     - What does it mean to search backwards from the goal?
     - predecessors
     - Needs one of then to be stored in memory

**** TODO Comparing search strategies
     - b: branching factor; d: depth of solution; m: max-depth of tree;
       l: depth limit
     - breadth-first
       - time: b^d
       - space: b^d
       - optimal: yes
       - complete: yes
     - uniform-cost
       - time: b^d
       - space: b^d
       - optimal: yes
       - complete: yes
     - depth-first
       - time: b^m
       - space: bm
       - optimal: No
       - complete: no
     - depth-limited
       - time: b^l
       - space: bl
       - optimal: no
       - complete: yes, if l > d
     - iterative-deepening
       - time: b^d
       - space: bd
       - optimal: yes
       - complete: yes
     - bidirectional
       - time: b^(d/2)
       - space: b^(d/2)
       - optimal: yes
       - complete: yes
*** TODO 3.6 Avoiding repeated states
    - Do not return to the state you came from
    - Do not create paths with cycles in them. Expand refuses to generate new
      nodes that is the same as ancestors
    - Do not generate any state that was ever generated before. Typically
      requires every state generated to be kept in memory.
*** TODO 3.7 Constraint satisfaction search
    States are defined by the values of a set of variables, and goal test
    specifies a set of constraints.

    - constraint satisfaction problem
    - set of variables: specifies the states
    - set of constraints: specifies the goal test
    - unary constraints
    - binary constraints
    - higher-order contraints
    - absolute contraints
    - preference constraints
    - domain of variables D_i: V_i in D_i; D_i discrete or continuous
    - unary constraint subsets domain
    - binary constraint subsets cross-product of domains

    - Order of variable assignments makes no difference: generate successors
      by choosing values for only a signel variable at each node.

    - Principal source of structure in CSPs: goal test decomposed into
      constraints

    - backtracking search: backtracks if algo finds that constraint not
      satisfied. Limitation: explores level even if domain of another
      variable is null.

    - forward checking: avoid exploring next level if future levels are
      all unsolvable: look ahead for unsolvability: delete from domains of
      yet uninstantiated variables conflicting values. If any become null set,
      backtrack immediately.

      - special case of arc consistency checking

      - arc consistency checking: every variable has a value in its domain
        consistent
        with constraints on that variable

      - constraint propagation

    - instanced RVs.

    - Arc consistency

*** TODO 3.8 Summary
*** TODO Bibliographical and historical notes
*** TODO Exercises
** TODO 4 Informed search methods
*** TODO 4.1 Best-first search
     We have an evaluation function that tells us how desirable it is to
     expand a node. If we order nodes in this manner, and expand nodes
     as such, it is called a best-first search.

     - evaluation function
     - best-first search

     #+BEGIN_SRC
     function Best-First-Search(problem, EVAL-FN) returns a solution sequence
     #+END_SRC

**** TODO Minimize estimated cost to reach a goal: greedy search
     - heuristic function :: returns estimated cost of the cheapest path from the
     state at node n to a goal state.

     - greedy search :: use h to select the next node to expand

     - h(n) = estimated cost of the cheapest path from the state
       at node n to a goal state

     We can use the straight-line distance as a heuristic.

     - h_{SLD}(n) = straight-line distance between n and the goal location

     #+BEGIN_SRC
     function Greedy-Search(problem) returns a solution or failure
       return Best-First-Search(problem, h)
     #+END_SRC

     - Straight-line distance to goal
     - May not be optimal
     - incomplete: can start down infinite path

**** TODO Minimizing the total path cost: A* search

     Consider a g(n) which is the cost of the path so far; let h(n)
     be a function that never overestimates the cost to the goal.
     We call h(n) an admissible heuristic. Let f = g(n) + h(n).

     - admissible heuristic: never overestimates; if h is admissible
       f never overestimates the actual cost of the best solution

     #+BEGIN_SRC
     function A*-Search(problem) returns a solution or failure
       return Best-First-Search(problem, g + h)
     #+END_SRC

***** The behavior of A* search

      - f is monotonic increasing
      - if h is not monotonic, we consider using h' where
        h'(n) = max(h(n), h'(n-1))
      - pathmax equation: f(n') = max(f(n), g(n')+h(n')), where n is parent
        of n'
      - distance are circular around start state

      The f-cost never decreases. If the heuristic is not monotonic, and
      a generated node's calculated f-cose is lower than its parents' use
      its parents' f-cost instead.

      A* search is optimally efficient for any given heuristic function.

***** Proof of the optimality of A*

***** Proof of the completeness of A*
      - A* is complete on locally finite graphs (finite branching factor)

***** Complexity of A*

      - Exponential growth will nevertheless occur unless the error in the
        heuristic function grows no faster than the logarithm of the actual path
        cost.

        - \|h(n) - h*(n)| <= O(log h*(n))

        where h*(n) is the true cost of getting from n to the goal.

        But space is A*'s main drawback as it keeps all generated nodes in
        memory.

*** 4.2 Heuristic functions
    - Consider 8-puzzle
      - h1: misplaced tiles
      - h2: Manhattan distance
**** TODO The effect of heuristic accuracy on performance
     - effective branching factor b*: given N nodes generated, this is the
       branching factor for N nodes at d depth if no heuristic were used
     - h2 dominates h1
       - always better to use heuristic with higher values that does not
         overestimate
**** TODO Inventing heuristic functions
**** TODO Heuristics for constraint satisfaction problems
*** TODO 4.3 Memory bounded search
**** TODO Iterative deepening A* search (IDA*)
**** TODO SMA* search
*** TODO 4.4 Iterative improvement algorithms
**** TODO Hill-climbing search
**** TODO Simulated annealing
**** TODO Applications in constraint satisfaction problems
*** TODO Summary
*** TODO Bibliographical and historical notes
*** TODO Exercises
** TODO 5 Game playing
*** TODO 5.1 Introduction: Dames as Search Problems
*** TODO 5.2 Perfect decisions in two-person games
*** TODO 5.3 Imperfect decisions
**** TODO Evaluation functions
**** TODO Cutting off search
*** TODO 5.4 Alpha-beta pruning
**** TODO Effectiveness of alpha-beta pruning
*** TODO 5.5 Games that include an element of chance
**** TODO Position evaluation in games with chance nodes
**** TODO Complexity of expectiminimax
*** TODO 5.6 State-of-the-art game programs
**** TODO Chess
**** TODO Checkers or draughts
**** TODO Othello
**** TODO Backgammon
**** TODO Go
*** TODO 5.7 Discussion
*** TODO 5.8 Summary
* TODO III Knowledge and reasoning
** TODO 6 Agents that reason logically
** TODO 7 First-order logic
** TODO 8 Building a knowledge base
** TODO 9 Inference in first-order logic
** TODO 10 Logical reasoning systems
* TODO IV Acting logically
** TODO 11 Planning
** TODO 12 Practical planning
** TODO 13 Planning and acting
* TODO V Uncertain knowledge and reasoning
** TODO 14 Uncertainty
** TODO 15 Probabilistic reasoning systems
** TODO 16 Making simple decisions
** TODO 17 Making complex decisions
* TODO VI Learning
** TODO 18 Learning from observations
** TODO 19 Learning in neural and belief networks
** TODO 20 Reinforcement learning
** TODO 21 Knowledge in learning
* TODO VII Communicating, perceiving, and acting
** TODO 22 Agents that communicate
** TODO 23 Practical natural language processing
** TODO 24 Perception
** TODO 25 Robotics
* TODO VIII Conclusions
** TODO 26 Philosophical foundations
** TODO 27 AI: present and future
* TODO Complexity analysis and O() notation
* TODO Notes on languages and algorithms
